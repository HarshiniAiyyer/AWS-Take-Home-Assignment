# ğŸ’³ Bank Transaction Pattern Detection System

A data processing pipeline for ingesting and analyzing banking transaction data to detect actionable patterns using PySpark on Databricks, AWS S3, and PostgreSQL.

---

## ğŸ§  Project Overview

This pipeline monitors transactional data in near real-time, splits it into chunks, logs them to AWS services, and performs pattern detection using Apache Spark (via Databricks).

---
## Downloadables
A downloadable zip file is present in the notebook as ```bucket```.

## ğŸ“Œ Use Case

The system is designed to detect the following customer behavior patterns:

1. **UPGRADE**  
   Customers in top 1% by transaction count and bottom 1% by customer importance (weight).  
   _Condition_: merchant transaction count > 50K

2. **CHILD**  
   Customers whose average transaction value is less than â‚¹23 and count â‰¥ 80.

3. **DEI-NEEDED**  
   Female customers with > 100 transactions and whose volume exceeds male customers.

---

## ğŸ—ï¸ Architecture

```mermaid
flowchart TD
    subgraph Ingestion
        X1["ğŸ“„ Google Drive: transactions.csv"]
        X2["ğŸ“„ Google Drive: CustomerImportance.csv"]
        P1["ğŸ” Transaction Splitter (10,000-row chunks/sec)"]
        S3["â˜ï¸ AWS S3: /chunks"]
        DB["ğŸ—ƒï¸ PostgreSQL (RDS): Logging"]
        X1 --> P1
        P1 --> S3
        P1 --> DB
    end

    subgraph Detection
        D1["ğŸ’» Databricks Cluster"]
        DF["ğŸ“„ CustomerImportance.csv -> DataFrame"]
        DETECT["ğŸ§  Pattern Detector"]
        OUT["â˜ï¸ S3: /detections"]
        S3 --> D1
        X2 --> DF --> D1
        DB -.-> D1
        D1 --> DETECT --> OUT
    end

    DETECT --> P2["âœ… Pattern 1: UPGRADE"]
    DETECT --> P3["âœ… Pattern 2: CHILD"]
    DETECT --> P4["âœ… Pattern 3: DEI-NEEDED"]
```

> ğŸ” Note: Although Databricks Community Edition doesnâ€™t support job scheduling, a job is depicted in the architecture to align with assignment requirements.

---

## ğŸ§° Tech Stack

| Tool | Role |
|------|------|
| **Databricks (CE)** | Spark processing & pattern detection |
| **PySpark** | Data transformation and detection logic |
| **AWS S3** | Input/output data storage (`/chunks`, `/detections`) |
| **AWS RDS PostgreSQL** | Logging processed chunks |
| **Google Drive** | Source of CSVs (`transactions.csv`, `CustomerImportance.csv`) |
| **VS Code** | Development and local data sync |

---

## ğŸ“ Folder Structure

```bash
takehome/
â”œâ”€â”€ bucket/
â”‚   â”œâ”€â”€ s3_chunks/         # Synced from s3://banktransactionskrnl1/chunks/
â”‚   â””â”€â”€ s3_detections/     # Synced from s3://banktransactionskrnl1/detections/
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ ingestion_notebook.py
â”‚   â””â”€â”€ pattern_detection_notebook.py
â”œâ”€â”€ sql/
â”‚   â””â”€â”€ logging_table_schema.sql
â””â”€â”€ README.md
```

---

## ğŸ”„ Syncing Data from S3

Ensure the AWS CLI is installed and run:

```bash
aws s3 sync s3://banktransactionskrnl1/chunks/ "D:\takehome\bucket\s3_chunks"
aws s3 sync s3://banktransactionskrnl1/detections/ "D:\takehome\bucket\s3_detections"
```

---

## ğŸ“‹ Status

- [x] Ingestion mechanism tested with `transactions.csv`
- [x] Logging integrated using AWS RDS PostgreSQL
- [x] Pattern logic implemented in Databricks using PySpark
- [x] Outputs successfully stored in `/detections` folder on S3
- [ ] Optional: Set up a job scheduler (not supported in CE)

---

## ğŸ§‘â€ğŸ’» Author

Harshini Aiyyer  
[GitHub](https://github.com/HarshiniAiyyer) â€¢ [LinkedIn](https://linkedin.com/in/harshini-aiyyer)
